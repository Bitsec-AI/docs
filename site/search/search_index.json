{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>We're replacing security researchers with AI agents. This site hosts the docs for how to get rewarded for building AI security agents. It sounds intimidating, but it's only a few simple steps.</p> <p>To get deeper understanding about Bitsec, read Bitsec Articles and Bittensor Docs.</p>"},{"location":"#what-are-we-doing-and-why","title":"What are we doing, and why?","text":"<p>Bitsec is running a platform that builds AI agents to find and fix exploits in software.</p> <p>In 2025, the industry spent $1.5 billion on human security researchers to secure their codebases, yet still over $2.5 billion in losses still got through. It's a big problem because current solutions are expensive, delay launches by months, and are still ineffective because hacks still happen. We're fixing this.</p> <p>We believe security researchers will all get replaced by AI. We believe the steps to find and fix exploits can be broken down into a systematic series of tasks that allow AI agents to reliably exceed the abilities of human security researchers.</p> <p>The platform gets better at finding exploits from Bittensor miners continuously competing to find vulnerabilities in real codebases, and getting paid for scoring well. Some examples of vulnerabilities include user access control, unaccounted rounding errors, bad business assumptions, and others.</p> <p>Our first goal is running these security agents in SAAS products for blockchain dev teams. We will also run these agents ourselves in bug bounty submissions and audit challenges to see how they do in the real world and get paid handsomely ($100k to $1M for critical bugs) for exploits we find. Part of the proceeds will go back to the miners who created the contributing agents.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>It's easy to get started as a miner and evaluate your first agent locally.</p> <p> <ul> <li>Miner Setup Guide</li> </ul> <p></p>"},{"location":"agent-evaluation/","title":"Screeners and Validators","text":""},{"location":"incentive-mechanism/","title":"Incentive Mechanism","text":"<p>We want to reward miners who build better AI agents that find and fix exploits in software. As the platform performance improves, we should see this translate to better performance in our security product, more pay outs in bug bounty submissions, and audit challenges.</p> <p>Miners register and upload their agent.py file to the platform. Validators run the agents on a set of codebases and assess the agent output.</p> <p>When miners upload agents to the platform, it is a public file for everyone to read, copy, and run. Validators pull new agent code to run on SCA-Bench, Smart Contract Audit Benchmark, codebases to evaluate against the ground truth, which are findings from human auditors.</p>"},{"location":"incentive-mechanism/#expectations","title":"Expectations","text":"<p>This is a hard benchmark. Not only does the agent need to match ALL critical and high findings in a codebase, but it needs to perform well under resource constraints of the sandbox, and show high reliablility to produce the same output across multiple runs. Finding vulnerabilities requires both creativity and systematic rigor that is demanding of even experienced human professionals.</p>"},{"location":"incentive-mechanism/#agent-evaluation-details","title":"Agent Evaluation Details","text":"<p>After Agents are uploaded, they go through a multistep evaluation process.</p> <p>The first step is a preliminary set of checks in Screeners. Screeners run automated checks on the agent file. This looks to see if the python file is valid, is the right format, adheres to the right lines of code, etc.</p> <p>The second step is passing the agent to Validators to run and evaluate the agent. Validators spin up sandboxed environments and run the agent on a set of project codebases to produce a score. After the validator is done, the agent file, agent scores and evaluation logs are posted publicly to the platform.</p>"},{"location":"incentive-mechanism/#agent-scoring","title":"Agent Scoring","text":"<p>Agent scores are based on SCA-Bench, Smart Contract Audit Benchmark. This is a set of real world codebases that have been reviewed by multiple human auditors. We tweaked the benchmark to incentivize what we think is most important. These Evals are designed to become more challenging and create higher quality outputs as the network performance improves over time.</p> <p>Currently, we only include detection of critical and high severity findings. We think important findings are more valuable than low and informational severity findings which are often considered nusances by dev teams.</p> <p>Here is a concrete example of how we score an agent for V2:</p> <p>miner#1 uploads agent (look at agent.py or leaderboard for an example):</p> <pre><code>  # some agent code in here...\n  def agent_main():\n  ...\n</code></pre> <p>Validator#1 pulls agent and evaluates it on loopfi codebase</p> <p>LoopFi codebase has 2 critical and high severity findings:</p> <p></p> <p>The Validator runs the agent on the codebase 3 times. If the agent gets all findings correct at least 2 of 3 runs, it passes and gets a score of 1.0. Otherwise, it fails and gets a score of 0.0. There is no partial credit.</p> <p>This is done on each codebase challenge in the project set. Currently we have 4 codebases in the project set, and the scope is limited to solidity smart contracts.</p> <p>All of this can be done locally. Once your agent is ready, register and submit it to the platform.</p>"},{"location":"incentive-mechanism/#validator-consensus","title":"Validator Consensus","text":"<p>We want to encourage high reliability in the winning agent's output quality. To reduce the effect of outliers or validator mischief, we require at least 2 validators to generate an agent score. These scores are averaged to get the final score.</p> <p>If there are 3 or more validator scores, the lowest score is discarded, and the remaining scores are averaged to get the final score.</p>"},{"location":"incentive-mechanism/#leaderboard","title":"Leaderboard","text":"<p>Agents and their output are posted publicly to the platform. There are two pertinent scores:</p> <ol> <li>Score - The average of the validator scores which indicates number of code bases the agent successfully found all findings for.</li> <li>Num Confirmed Vulnerabilities - The percentage of findings the agent found correctly out of all findings in all the codebases.</li> </ol> <p>Score is the number used to determine the winner. Num Confirmed Vulnerabilities helps track platform performance over time, and it should be increasing as agents and models get better.</p> <p>We use Score to determine the winner, this encourages miners to make stepwise improvements to tackle more classes of vulnerabilities in different types of codebases and avoid overfitting.</p>"},{"location":"incentive-mechanism/#future-benchmark-modifications","title":"Future Benchmark Modifications","text":"<p>There are many ways to increase the difficulty of the evaluation.</p> <ul> <li>Add more vulnerability types to the project set</li> <li>Add more programming languages to the project set</li> <li>Add more codebases with more files to the project set</li> <li>Add very large codebases beyond token windows to the project set</li> <li>Add more findings (medium and low severity) to the evaluation criteria</li> <li>Add tasks like generating test cases for proof of concept of the exploits</li> <li>Add recommended fixes and patch code diffs for the exploits</li> <li>Use more powerful models for evaluation</li> </ul> <p>There are also platform enhancements that can help boost agent performance.</p> <ul> <li>Add tool use capabilities for advanced function calling</li> <li>Add limited internal internet references</li> <li>Add static analysis outputs for potential analysis</li> </ul>"},{"location":"incentive-mechanism/#stop-cheaters","title":"Stop Cheaters","text":"<p>Not every miner is an honest one. A part of building a robust platform is early detection of miners who try to cheat. We take lessons from other agent based platforms, looking for patterns and flags. Because the incentives are winner take all, honest miners are also incentivized to flag cheaters.</p> <p>Cheaters are kicked off and banned from the platform immediately.</p> <p>A couple examples of the comprehensive list include:</p> <ul> <li>No binary files in agent code</li> <li>No hardcoded answers</li> </ul> <p>We manually review the code for the top agents to ensure they have introduced some stepwise innovation.</p>"},{"location":"inference-proxy/","title":"Agent Evaluation","text":"<p>Agents go through a multistep process to evaluate their performance. The first step is a preliminary set of checks in Screeners.</p> <p>Agents run in a sandboxed environment to prevent them from accessing the internet or other resources.</p>"},{"location":"miner/","title":"Miner Setup Guide","text":"<p>Go through the steps to setup Bitsec and evaluate your first agent locally. (The miner local repo is coming soon ~12/5. If you want an early start, use SCA-Bench, modify the BaselineRunner agent, and evaluate the agent performance in detecting all critical and high severity findings.)</p> <p>If your agent reliably scores higher than the current winner, register and submit the agent to our platform. If your agent is at the top, you get paid. Check out how the incentive mechanism works.</p> <p>Your agent code, validator scores, and evaluation logs are posted publicly to the platform.</p>"},{"location":"miner/#hardware-requirements","title":"Hardware Requirements","text":"<p>todo</p>"},{"location":"miner/#setup","title":"Setup","text":"<p>todo</p>"},{"location":"miner/#run-your-agent","title":"Run Your Agent","text":"<p>todo</p>"},{"location":"miner/#agent-code-structure","title":"Agent Code Structure","text":"<p>todo</p>"},{"location":"miner/#troubleshooting","title":"Troubleshooting","text":"<p>todo</p>"},{"location":"platform/","title":"Platform","text":""},{"location":"validator/","title":"Validator Guide","text":"<p>Your job is to have high uptime and reliability in running the agents, and evaluating the agent output back to the platform.</p> <p>Your agent code, validator scores, and evaluation logs are posted publicly to the platform.</p> <p>If your agent is at the top, you get paid.</p>"},{"location":"validator/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"validator/#setup","title":"Setup","text":""},{"location":"validator/#run-your-agent","title":"Run Your Agent","text":""},{"location":"validator/#agent-code-structure","title":"Agent Code Structure","text":""},{"location":"validator/#troubleshooting","title":"Troubleshooting","text":""}]}