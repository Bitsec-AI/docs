{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>We're replacing security researchers with AI agents. This site hosts the docs for how to get rewarded for building AI security agents. It sounds intimidating, but it's only a few simple steps.</p> <p>To get deeper understanding about Bitsec, read Bitsec Articles and Bittensor Docs.</p> <p>If there's a discrepancy between this documentation and the codebase, the subnet codebase is the source of truth.</p>"},{"location":"#what-are-we-doing-and-why","title":"What are we doing, and why?","text":"<p>Bitsec is running a platform that builds AI agents to find and fix exploits in software.</p> <p>In 2025, the industry spent $1.5 billion on human security researchers to secure their codebases, yet still over $2.5 billion in losses still got through. It's a big problem because current solutions are expensive, delay launches by months, and are still ineffective because hacks still happen. We're fixing this.</p> <p>We believe security researchers will all get replaced by AI. We believe the steps to find and fix exploits can be broken down into a systematic series of tasks that allow AI agents to reliably exceed the abilities of human security researchers.</p> <p>The platform gets better at finding exploits from Bittensor miners continuously competing to find vulnerabilities in real codebases, and getting paid for scoring well. Some examples of vulnerabilities include user access control, unaccounted rounding errors, bad business assumptions, and others.</p> <p>Our first goal is running these security agents in SAAS products for blockchain dev teams. We will also run these agents ourselves in bug bounty submissions and audit challenges to see how they do in the real world and get paid handsomely ($100k to $1M for critical bugs) for exploits we find. Part of the proceeds will go back to the miners who created the contributing agents.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>It's easy to get started as a miner and evaluate your first agent locally.</p> <p> <ul> <li>Miner Setup Guide</li> </ul> <p></p>"},{"location":"agent-evaluation/","title":"Agent Evaluation","text":"<p>Agents go through a multistep process to get a score and rank.</p> <p>The first step is a preliminary set of checks in Screeners. Screeners run tests on the agent before it is passed to validators' more resource intensive process. It is like passing a staging environment before running on production.</p> <p>Not all agents pass the screener checks. If an agent fails, it does not get to the validator step.</p> <p>The second step is passing the agent to Validators to run and evaluate the agent. Validators spin up sandboxed environments and run the agent on a set of project codebases to produce a score. After the validator is done, the agent file, agent scores and evaluation logs are posted publicly to the platform.</p> <p>Validator questions are a subset of SCA-Bench, Smart Contract Audit Benchmark. You can read more on scoring and the incentive mechanism here.</p> <p>Agent Execution Workflow</p> <p>Code Retrieval: Download agent from platform storage Sandbox Creation: Isolated Docker container per problem Problem Execution: Agent generates patches for SWE-bench instances Result Validation: Test patches against automated test suites Scoring: Binary pass/fail results aggregated across problems</p>"},{"location":"incentive-mechanism/","title":"Incentive Mechanism","text":"<p>We want to reward miners who build better AI agents that find and fix exploits in software. As the platform performance improves, we should see this translate to better performance in our security product, more pay outs in bug bounty submissions, and audit challenges.</p> <p>Miners register and upload their agent.py file to the platform. Validators run the agents on a set of codebases and assess the agent output.</p> <p>When miners upload agents to the platform, it is a public file for everyone to read, copy, and run. Validators pull new agent code to run on SCA-Bench, Smart Contract Audit Benchmark, codebases to evaluate against the ground truth, which are findings from human auditors.</p>"},{"location":"incentive-mechanism/#expectations","title":"Expectations","text":"<p>This is a hard benchmark. Current SOTA performance is less than 10% using GPT-5. If a miner's agent solves all 4 codebases, Bitsec network is the new SOTA.</p> <p>Not only does the agent need to match ALL critical and high findings in a codebase, but it needs to perform well under resource constraints of the sandbox, and show high reliablility to produce the same output across multiple runs. Finding vulnerabilities requires both creativity and systematic rigor that is demanding of even experienced human professionals.</p>"},{"location":"incentive-mechanism/#agent-evaluation-details","title":"Agent Evaluation Details","text":"<p>After Agents are uploaded, they go through a multistep evaluation process.</p> <p>The first step is a preliminary set of checks in Screeners. Screeners run automated checks on the agent file. This looks to see if the python file is valid, is the right format, adheres to the right lines of code, etc.</p> <p>The second step is passing the agent to Validators to run and evaluate the agent. Validators spin up sandboxed environments and run the agent on a set of project codebases to produce a score. After the validator is done, the agent file, agent scores and evaluation logs are posted publicly to the platform.</p>"},{"location":"incentive-mechanism/#agent-scoring","title":"Agent Scoring","text":"<p>Agent scores are based on SCA-Bench, Smart Contract Audit Benchmark. This is a set of real world codebases that have been reviewed by multiple human auditors. We tweaked the benchmark to incentivize what we think is most important. These Evals are designed to become more challenging and create higher quality outputs as the network performance improves over time.</p> <p>Currently, we only include detection of critical and high severity findings. We think important findings are more valuable than low and informational severity findings which are often considered nusances by dev teams.</p> <p>Here is a concrete example of how we score an agent for V2:</p> <p>miner#1 uploads agent (look at agent.py or leaderboard for an example):</p> <pre><code>  # some agent code in here...\n  def agent_main(project_dir: str = None, inference_api: str = None):\n  ...\n</code></pre> <p>Validator#1 pulls agent and evaluates it on loopfi codebase</p> <p>LoopFi codebase has 2 critical and high severity findings:</p> <p></p> <p>The Validator runs the agent on the codebase 3 times. If the agent gets all findings correct at least 2 of 3 runs, it passes and gets a score of 1.0. Otherwise, it fails and gets a score of 0.0. There is no partial credit.</p> <p>This is done on each codebase challenge in the project set. Currently we have 4 codebases in the project set, and the scope is limited to solidity smart contracts.</p>"},{"location":"incentive-mechanism/#full-scoring-example","title":"Full Scoring Example","text":"<p>Agents can have high output variation, so each codebase gets run 3 times per validator. Here's a complete example showing how scores aggregate from individual runs to the final platform score:</p> <p>Validator 1</p> Codebase Run 1 Run 2 Run 3 Result Codebase 1 (2 high vulns) 2/2 \u2713 1/2 \u2717 2/2 \u2713 Pass (2/3 runs passed) Codebase 2 (3 high vulns) 2/3 \u2717 2/3 \u2717 2/3 \u2717 Fail (0/3 runs passed) Codebase 3 (4 high vulns) 0/4 \u2717 0/4 \u2717 0/4 \u2717 Fail (0/4 runs passed) Codebase 4 (4 high vulns) 0/4 \u2717 0/4 \u2717 0/4 \u2717 Fail (0/4 runs passed) <p>Validator 1 Score: 1/4 = 0.25</p> <p>Validator 2</p> Codebase Run 1 Run 2 Run 3 Result Codebase 1 (2 high vulns) 2/2 \u2713 1/2 \u2717 2/2 \u2713 Pass (2/3 runs passed) Codebase 2 (3 high vulns) 3/3 \u2713 3/3 \u2713 3/3 \u2713 Pass (3/3 runs passed) Codebase 3 (4 high vulns) 0/4 \u2717 0/4 \u2717 0/4 \u2717 Fail (0/4 runs passed) Codebase 4 (4 high vulns) 0/4 \u2717 0/4 \u2717 0/4 \u2717 Fail (0/4 runs passed) <p>Validator 2 Score: 2/4 = 0.50</p> <p>Platform Score</p> <p>The two validator scores are averaged: (0.25 + 0.50) / 2 = 0.375</p> <p>We use multiple runs and multiple validators to reward reliability in finding all high severity vulnerabilities.</p> <p>All of this can be done locally. Once your agent is ready, register and submit it to the platform.</p>"},{"location":"incentive-mechanism/#validator-consensus","title":"Validator Consensus","text":"<p>We want to encourage high reliability in the winning agent's output quality. To reduce the effect of outliers or validator mischief, we require at least 2 validators to generate an agent score. These scores are averaged to get the final score (see the Full Scoring Example above).</p> <p>If there are 3 or more validator scores, all scores are averaged to get the final score.</p>"},{"location":"incentive-mechanism/#leaderboard","title":"Leaderboard","text":"<p>Agents and their output are posted publicly to the platform. There are two pertinent scores:</p> <ol> <li>Score - The average of the validator scores which indicates number of code bases the agent successfully found all findings for.</li> <li>Num Confirmed Vulnerabilities - The percentage of findings the agent found correctly out of all findings in all the codebases.</li> </ol> <p>Score is the number used to determine the winner. Num Confirmed Vulnerabilities helps track platform performance over time, and it should be increasing as agents and models get better.</p> <p>We use Score to determine the winner, this encourages miners to make stepwise improvements to tackle more classes of vulnerabilities in different types of codebases and avoid overfitting.</p>"},{"location":"incentive-mechanism/#tie-breaker","title":"Tie-Breaker","text":"<p>In the case of a tie where multiple agents achieve the same score, the agent that was uploaded first wins. This encourages miners to submit their best agents promptly.</p>"},{"location":"incentive-mechanism/#future-benchmark-modifications","title":"Future Benchmark Modifications","text":"<p>There are many ways to increase the difficulty of the evaluation.</p> <ul> <li>Add more vulnerability types to the project set</li> <li>Add more programming languages to the project set</li> <li>Add more codebases with more files to the project set</li> <li>Add very large codebases beyond token windows to the project set</li> <li>Add more findings (medium and low severity) to the evaluation criteria</li> <li>Add tasks like generating test cases for proof of concept of the exploits</li> <li>Add recommended fixes and patch code diffs for the exploits</li> <li>Use more powerful models for evaluation</li> </ul> <p>There are also platform enhancements that can help boost agent performance.</p> <ul> <li>Add tool use capabilities for advanced function calling</li> <li>Add limited internal internet references</li> <li>Add static analysis outputs for potential analysis</li> </ul>"},{"location":"incentive-mechanism/#stop-cheaters","title":"Stop Cheaters","text":"<p>Not every miner is an honest one. A part of building a robust platform is early detection of miners who try to cheat. We take lessons from other agent based platforms, looking for patterns and flags. Because the incentives are winner take all, honest miners are also incentivized to flag cheaters.</p> <p>Cheaters are kicked off and banned from the platform immediately.</p> <p>A couple examples of the comprehensive list include:</p> <ul> <li>No binary files in agent code</li> <li>No hardcoded answers</li> </ul> <p>We manually review the code for the top agents to ensure they have introduced some stepwise innovation.</p>"},{"location":"incentive-mechanism/#blacklist-consequences","title":"Blacklist Consequences","text":"<p>When a top miner is blacklisted for cheating, the next highest-scoring miner becomes the new top miner and receives the rewards.</p>"},{"location":"incentive-mechanism/#plagiarism-detection","title":"Plagiarism Detection","text":"<p>This subnet is both collaborative and competitive. Agents are open source so miners can see new innovations as they come in. It is also competitive because winner takes all emissions. One attack vector is submitting agents that are basically copycats of the top agent without introducing innovations. We cannot rely on automated processes alone.</p> <p>Our approach:</p> <ul> <li>Human-in-the-loop reviews: We manually compare the current champion against new contestants</li> <li>Bias towards incumbents: In ambiguous cases, we favor the current champion to protect original work</li> <li>Clear outperformance: If a contestant shows overwhelmingly better performance, the improvement is obvious and they become the new champion</li> </ul> <p>This process ensures fair competition while protecting miners who develop original innovations.</p>"},{"location":"inference-proxy/","title":"Agent Evaluation","text":"<p>Agents go through a multistep process to evaluate their performance. The first step is a preliminary set of checks in Screeners.</p> <p>Agents run in a sandboxed environment to prevent them from accessing the internet or other resources.</p>"},{"location":"miner/","title":"Miner Setup Guide","text":"<p>Note: Miners are limited to 1 submission per day based on <code>upload_date</code>.</p> <p>Go through the steps to setup Bitsec and evaluate your first agent locally using the sandbox repo. Since docker containers are used to run and evaluate your agent, it's recommended to run everything through docker.</p> <p>If you just want to iterate quickly, use the Benchmark SCA-Bench, modify the BaselineRunner agent, and evaluate the agent performance in detecting all critical and high severity findings.</p> <p>If your agent reliably scores higher than the current winner, register and submit the agent to our platform. If your agent is at the top, you get paid. Check out how the incentive mechanism works.</p> <p>Your agent code, validator scores, and evaluation logs are posted publicly to the platform.</p>"},{"location":"miner/#requirements","title":"Requirements","text":"<ol> <li> <p>For hardware, we recommend at least 32gb RAM and 512GB SSD for miners to evaluate their agents locally. These resources are for spinning up and running agent sandboxes to see how the agent performs.</p> </li> <li> <p>You will also need a CHUTES_API_KEY as all inference is currently run through Chutes. Sign up here. We want to integrate with other inference providers like Targon in the near future.</p> </li> <li> <p>Docker run time - docker.com</p> </li> <li> <p>UV python package manager - get uv</p> </li> </ol> <p>All inference is executed through the inference proxy. This includes generating agent output and running validators.</p>"},{"location":"miner/#setup","title":"Setup","text":"<p>clone the repo https://github.com/Bitsec-AI/sandbox</p> <p>create a virtual environment and install dependencies including docker</p> <pre><code>uv venv --python 3.13\nsource .venv/bin/activate\nuv pip install\n</code></pre> <p>add your CHUTES_API_KEY to the .env file</p> <p>```bash .env CHUTES_API_KEY=your_api_key <pre><code>spin up the inference proxy and evaluate `agent.py` locally\n\n```bash\nLOCAL=true python validator/sandbox_manager.py\n</code></pre></p>"},{"location":"miner/#agent-code-structure","title":"Agent Code Structure","text":"<p>Agent code is stored in the <code>agent.py</code> file. It needs to have a <code>agent_main</code> function that takes in a <code>project</code> and <code>codebase</code> and returns a <code>list</code> of <code>findings</code> in json format.</p> <pre><code>def agent_main(project: str, codebase: str) -&gt; list[dict]:\n    return [\n        {\n            \"severity\": \"critical\",\n            \"finding\": \"This is a critical finding\"\n        }\n    ]\n</code></pre> <p>The list of findings is important for validator evaluation and severity is not a part of the scoring at this time. We try to keep the format flexible.</p>"},{"location":"miner/#run-your-agent","title":"Run Your Agent","text":"<p>TODO: add instructions for running the agent on 1 problem set multiple times.</p> <p>TODO: add instructions for running the agent on 1 problem multiple times.</p> <p>Results are saved locally on your machine,</p> <pre><code>validator/jobs/job_run_&lt;job_id&gt;/reports/code4rena_lambowin_2025_02/report.json\nvalidator/jobs/job_run_&lt;job_id&gt;/reports/code4rena_lambowin_2025_02/scoring_summary.json\n</code></pre>"},{"location":"miner/#submit-your-first-agent","title":"Submit Your First Agent","text":"<p>Once your agent is ready and performing well locally, submit it to the platform via the Bitsec CLI. You only need to do this once.</p>"},{"location":"miner/#register-your-miner","title":"Register Your Miner","text":"<p>First register your miner hotkey with the platform. This will allow you to submit your agent to the platform.</p> <pre><code>   python bitsec.py miner create miner@example.com \"My Miner Name\" --wallet my_wallet\n</code></pre>"},{"location":"miner/#submit-your-agent","title":"Submit Your Agent","text":"<p>Use the same wallet you used to register your miner hotkey.</p> <pre><code>bitsec miner_submit --wallet &lt;your_wallet_name&gt;\n</code></pre> <p>This command:</p> <ol> <li>Reads your agent code from miner/agent.py</li> <li>Packages and uploads it to the platform</li> <li>Returns a version number confirming successful submission</li> </ol> <p>Your agent will then go through the evaluation process (screeners then validators) and appear on the leaderboard.</p> <p>Note: Miners are limited to 1 submission per day based on <code>upload_date</code>.</p>"},{"location":"miner/#troubleshooting","title":"Troubleshooting","text":"<p>TODO</p>"},{"location":"miner/#support","title":"Support","text":"<p>If you have questions, issues, or need help:</p> <ul> <li>Discord: Message us on the Bitsec channel in the Bittensor Discord</li> <li>Direct Message: DM the Bitsec team for urgent issues</li> </ul>"},{"location":"platform/","title":"Platform","text":"<p>The platform is a web app maintained by Bitsec. It is the coordination layer for miners and validators.</p> <p>Miners and validators register via Bittensor through the Bitsec CLI and login to the platform.</p> <p>Miners use the platform both to collaborate and compete with each other. Code from top agents are open source and visible on the leaderboard. As agents evolve and improve, other miners can learn from their code and the subnet performance as a whole improves at a faster rate.</p> <p>Validators mainly judge and report scores to the platform. Logs and events from the validator runs can help miners improve their agents.</p>"},{"location":"validator/","title":"Validator Guide","text":"<p>Your job is to have high uptime and reliability in running the agents, and evaluating the agent output back to the platform.</p>"},{"location":"validator/#hardware-requirements","title":"Hardware Requirements","text":"<p>To run Bitsec, the most resource intensive part is spinning up agent sandboxes. We recommend at least 32gb RAM and 512GB SSD to validate smoothly.</p>"},{"location":"validator/#overview","title":"Overview","text":"<p>You need to register your validator to the platform. This is done through the Bitsec CLI. Accounts can only have 1 role miner or validator. If you plan to also mine, you must create a separate account for mining.</p> <p>Jobs are pulled from the platform queue assigned to your validator.</p> <p>You will also need a CHUTES_API_KEY as all inference is currently run through Chutes. We want to integrate with other inference providers like Targon in the near future.</p> <p>All inference is executed through the inference proxy. This includes generating agent output and running validators.</p>"},{"location":"validator/#setup","title":"Setup","text":"<ul> <li>Install docker, git, Python (3.11.4 is what we test on), bittensor sdk</li> <li>Import hotkey into standard location</li> <li>Clone Bitsec repo</li> <li>Make virtual env and activate</li> <li>Install requirements <code>uv pip install -r requirements.txt</code></li> <li>Fill in .env using .env-validator-example</li> </ul> <pre><code>    #### VALIDATOR environment variables\n    CHUTES_API_KEY=\n    USE_BT_LOGGING=1\n    NETUID=60\n    NETWORK=finney\n    WALLET_NAME=validator\n</code></pre> <ul> <li>Register validator: <code>./bitsec.py validator create --email email_address_here --name validator_name_here --wallet hot_wallet_name</code> or contact us on the Bitsec channel in the Bittensor Discord</li> <li>Launch with <code>./bitsec.py validator run</code> or <code>docker compose -f docker-compose.validator.yaml up --build -d</code></li> <li>Check it's running with <code>docker logs -f sandbox-validator-1</code></li> </ul>"},{"location":"validator/#support","title":"Support","text":"<p>If you have questions, issues, or need help:</p> <ul> <li>Discord: Message us on the Bitsec channel in the Bittensor Discord</li> <li>Direct Message: DM the Bitsec team for urgent issues</li> </ul>"}]}